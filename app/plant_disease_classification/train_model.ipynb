{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/malgosielska/Studies/Sem 1 TAI/Projekt wdrozeniowy/plantie-backend/plantvillage_dataset/train',\n",
       " '/Users/malgosielska/Studies/Sem 1 TAI/Projekt wdrozeniowy/plantie-backend/plantvillage_dataset/val')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = \"/Users/malgosielska/Studies/Sem 1 TAI/Projekt wdrozeniowy/plantie-backend/plantvillage_dataset/train\"\n",
    "test_dir = \"/Users/malgosielska/Studies/Sem 1 TAI/Projekt wdrozeniowy/plantie-backend/plantvillage_dataset/val\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.Resize(size=(28, 28)),\n",
    "]) # maybe we need to resize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 43444\n",
       "     Root location: /Users/malgosielska/Studies/Sem 1 TAI/Projekt wdrozeniowy/plantie-backend/plantvillage_dataset/train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "                Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=True)\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 10861\n",
       "     Root location: /Users/malgosielska/Studies/Sem 1 TAI/Projekt wdrozeniowy/plantie-backend/plantvillage_dataset/val\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "                Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=True)\n",
       "            ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=transform)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                  transform=transform)\n",
    "\n",
    "train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Apple___Apple_scab': 0,\n",
       "  'Apple___Black_rot': 1,\n",
       "  'Apple___Cedar_apple_rust': 2,\n",
       "  'Apple___healthy': 3,\n",
       "  'Blueberry___healthy': 4,\n",
       "  'Cherry_(including_sour)___Powdery_mildew': 5,\n",
       "  'Cherry_(including_sour)___healthy': 6,\n",
       "  'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7,\n",
       "  'Corn_(maize)___Common_rust_': 8,\n",
       "  'Corn_(maize)___Northern_Leaf_Blight': 9,\n",
       "  'Corn_(maize)___healthy': 10,\n",
       "  'Grape___Black_rot': 11,\n",
       "  'Grape___Esca_(Black_Measles)': 12,\n",
       "  'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13,\n",
       "  'Grape___healthy': 14,\n",
       "  'Orange___Haunglongbing_(Citrus_greening)': 15,\n",
       "  'Peach___Bacterial_spot': 16,\n",
       "  'Peach___healthy': 17,\n",
       "  'Pepper,_bell___Bacterial_spot': 18,\n",
       "  'Pepper,_bell___healthy': 19,\n",
       "  'Potato___Early_blight': 20,\n",
       "  'Potato___Late_blight': 21,\n",
       "  'Potato___healthy': 22,\n",
       "  'Raspberry___healthy': 23,\n",
       "  'Soybean___healthy': 24,\n",
       "  'Squash___Powdery_mildew': 25,\n",
       "  'Strawberry___Leaf_scorch': 26,\n",
       "  'Strawberry___healthy': 27,\n",
       "  'Tomato___Bacterial_spot': 28,\n",
       "  'Tomato___Early_blight': 29,\n",
       "  'Tomato___Late_blight': 30,\n",
       "  'Tomato___Leaf_Mold': 31,\n",
       "  'Tomato___Septoria_leaf_spot': 32,\n",
       "  'Tomato___Spider_mites Two-spotted_spider_mite': 33,\n",
       "  'Tomato___Target_Spot': 34,\n",
       "  'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35,\n",
       "  'Tomato___Tomato_mosaic_virus': 36,\n",
       "  'Tomato___healthy': 37},\n",
       " 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict, len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x13c0d1b80>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x13bff7230>,\n",
       " 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "train_loader, test_loader, NUM_WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.0596e-02,  1.1013e-02,  3.6671e-03,  ..., -3.8001e-02,\n",
       "           -2.7272e-02, -2.9280e-02],\n",
       "          [ 1.7149e-02,  1.4004e-02,  3.7120e-03,  ..., -3.5130e-02,\n",
       "           -2.8804e-02, -1.5381e-02],\n",
       "          [ 1.5961e-02,  1.3483e-02,  1.1774e-04,  ..., -4.7084e-02,\n",
       "           -3.4970e-02, -1.7318e-02],\n",
       "          ...,\n",
       "          [-3.9131e-01, -5.1561e-01, -6.7428e-01,  ..., -3.0862e-01,\n",
       "           -2.8105e-01, -2.5933e-01],\n",
       "          [-4.4858e-01, -4.9426e-01, -6.2021e-01,  ..., -3.1413e-01,\n",
       "           -2.9239e-01, -2.8838e-01],\n",
       "          [-5.8006e-01, -5.7658e-01, -6.1283e-01,  ..., -3.2462e-01,\n",
       "           -3.1566e-01, -3.1754e-01]],\n",
       " \n",
       "         [[-3.4306e-02, -3.7273e-02, -2.8429e-02,  ..., -6.2032e-02,\n",
       "           -6.2077e-02, -6.8495e-02],\n",
       "          [-3.7753e-02, -3.4282e-02, -2.8385e-02,  ..., -6.3450e-02,\n",
       "           -6.4849e-02, -5.4596e-02],\n",
       "          [-3.8941e-02, -3.4803e-02, -3.1979e-02,  ..., -8.5833e-02,\n",
       "           -7.4050e-02, -5.6533e-02],\n",
       "          ...,\n",
       "          [-4.6992e-01, -4.7072e-01, -5.3646e-01,  ..., -3.5294e-01,\n",
       "           -3.3753e-01, -3.1472e-01],\n",
       "          [-5.7359e-01, -5.8119e-01, -6.0994e-01,  ..., -3.7668e-01,\n",
       "           -3.5406e-01, -3.5040e-01],\n",
       "          [-7.0799e-01, -7.0197e-01, -7.2173e-01,  ..., -3.8616e-01,\n",
       "           -3.7721e-01, -3.7909e-01]],\n",
       " \n",
       "         [[ 3.6569e-01,  3.6052e-01,  3.6397e-01,  ...,  3.8481e-01,\n",
       "            3.7934e-01,  3.7072e-01],\n",
       "          [ 3.6225e-01,  3.6351e-01,  3.6401e-01,  ...,  3.8572e-01,\n",
       "            3.8036e-01,  3.8903e-01],\n",
       "          [ 3.6106e-01,  3.6299e-01,  3.6042e-01,  ...,  3.6882e-01,\n",
       "            3.8044e-01,  3.9789e-01],\n",
       "          ...,\n",
       "          [-2.8828e-01, -5.6158e-01, -7.3871e-01,  ..., -2.6751e-02,\n",
       "           -4.7380e-03,  1.4934e-02],\n",
       "          [-3.2745e-01, -4.1954e-01, -6.2320e-01,  ..., -5.4201e-02,\n",
       "           -3.0738e-02, -2.6598e-02],\n",
       "          [-4.7480e-01, -4.7709e-01, -5.4161e-01,  ..., -9.1733e-02,\n",
       "           -8.2766e-02, -8.4642e-02]]]),\n",
       " 0,\n",
       " torch.Size([3, 28, 28]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "\n",
    "image, label, image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5) # (28 - 5) / 1 + 1 = 24\n",
    "        # (12, 24, 24)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        # (12, 12, 12)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5) # (12 - 5) / 1 + 1 = 8\n",
    "        # (24, 8, 8) -> (24, 4, 4) -> Flatten(24 * 4 * 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(24 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 38) # 38 is the number of classes, make it a parameter\n",
    "\n",
    "    def forward(self,  x: torch.Tensor):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Loss: 3.5851\n",
      "Epoch 2 / 5\n",
      "Loss: 3.4349\n",
      "Epoch 3 / 5\n",
      "Loss: 3.1765\n",
      "Epoch 4 / 5\n",
      "Loss: 2.6948\n",
      "Epoch 5 / 5\n",
      "Loss: 2.0765\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1} / 5\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"plant_disease_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=384, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet()\n",
    "net.load_state_dict(torch.load(\"plant_disease_model.pth\"))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 51.88%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Apple___Apple_scab',\n",
       " 'Apple___Black_rot',\n",
       " 'Apple___Cedar_apple_rust',\n",
       " 'Apple___healthy',\n",
       " 'Blueberry___healthy',\n",
       " 'Cherry_(including_sour)___Powdery_mildew',\n",
       " 'Cherry_(including_sour)___healthy',\n",
       " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
       " 'Corn_(maize)___Common_rust_',\n",
       " 'Corn_(maize)___Northern_Leaf_Blight',\n",
       " 'Corn_(maize)___healthy',\n",
       " 'Grape___Black_rot',\n",
       " 'Grape___Esca_(Black_Measles)',\n",
       " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
       " 'Grape___healthy',\n",
       " 'Orange___Haunglongbing_(Citrus_greening)',\n",
       " 'Peach___Bacterial_spot',\n",
       " 'Peach___healthy',\n",
       " 'Pepper,_bell___Bacterial_spot',\n",
       " 'Pepper,_bell___healthy',\n",
       " 'Potato___Early_blight',\n",
       " 'Potato___Late_blight',\n",
       " 'Potato___healthy',\n",
       " 'Raspberry___healthy',\n",
       " 'Soybean___healthy',\n",
       " 'Squash___Powdery_mildew',\n",
       " 'Strawberry___Leaf_scorch',\n",
       " 'Strawberry___healthy',\n",
       " 'Tomato___Bacterial_spot',\n",
       " 'Tomato___Early_blight',\n",
       " 'Tomato___Late_blight',\n",
       " 'Tomato___Leaf_Mold',\n",
       " 'Tomato___Septoria_leaf_spot',\n",
       " 'Tomato___Spider_mites Two-spotted_spider_mite',\n",
       " 'Tomato___Target_Spot',\n",
       " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
       " 'Tomato___Tomato_mosaic_virus',\n",
       " 'Tomato___healthy']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0 \n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on the test images: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Tomato___Early_blight\n",
      "Predicted class: Apple___Black_rot\n"
     ]
    }
   ],
   "source": [
    "new_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = new_transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "image_paths = [\"0a14783a-838a-4d4f-a671-ff98011714c6___FREC_Scab 3288.JPG\", \"0b8dabb7-5f1b-4fdc-b3fa-30b289707b90___JR_FrgE.S 3047.JPG\"]\n",
    "images = [load_image(path) for path in image_paths]\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for image in images:\n",
    "        output = net(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        print(f\"Predicted class: {class_names[predicted.item()]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
